{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StrikeWade\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#Import MNIST data\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# mnist = input_data.read_data_sets('../Tensorflow Examples Models/mnist', one_hot=True)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#Clear TF memory\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ToySequenceData(object):\n",
    "    \"\"\" Generate sequence of data with dynamic length.\n",
    "    This class generate samples for training:\n",
    "    - Class 0: linear sequences (i.e. [0, 1, 2, 3,...])\n",
    "    - Class 1: random sequences (i.e. [1, 3, 10, 7,...])\n",
    "\n",
    "    NOTICE:\n",
    "    We have to pad each sequence to reach 'max_seq_len' for TensorFlow\n",
    "    consistency (we cannot feed a numpy array with inconsistent\n",
    "    dimensions). The dynamic calculation will then be perform thanks to\n",
    "    'seqlen' attribute that records every actual sequence length.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3,\n",
    "                 max_value=1000):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        for i in range(n_samples):\n",
    "            # Random sequence length\n",
    "            len = random.randint(min_seq_len, max_seq_len)\n",
    "            # Monitor sequence length for TensorFlow dynamic calculation\n",
    "            self.seqlen.append(len)\n",
    "            # Add a random or linear int sequence (50% prob)\n",
    "            if random.random() < .5:\n",
    "                # Generate a linear sequence\n",
    "                rand_start = random.randint(0, max_value - len)\n",
    "                s = [[float(i)/max_value] for i in\n",
    "                     range(rand_start, rand_start + len)]\n",
    "                # Pad sequence for dimension consistency\n",
    "                s += [[0.] for i in range(max_seq_len - len)]\n",
    "                self.data.append(s)\n",
    "                self.labels.append([1., 0.])\n",
    "            else:\n",
    "                # Generate a random sequence\n",
    "                s = [[float(random.randint(0, max_value))/max_value]\n",
    "                     for i in range(len)]\n",
    "                # Pad sequence for dimension consistency\n",
    "                s += [[0.] for i in range(max_seq_len - len)]\n",
    "                self.data.append(s)\n",
    "                self.labels.append([0., 1.])\n",
    "        self.batch_id = 0\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        \"\"\" Return a batch of data. When dataset end is reached, start over.\n",
    "        \"\"\"\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        return batch_data, batch_labels, batch_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Params\n",
    "learning_rate = 0.01\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "#Network Params\n",
    "seq_max_len = 20 #sequence max length\n",
    "num_hidden = 64 #hidden layer num of features\n",
    "num_classes = 2 #linear sequence or not\n",
    "\n",
    "#TF graph input\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, seq_max_len, 1])\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])\n",
    "\n",
    "trainset = ToySequenceData(n_samples=1000, max_seq_len=seq_max_len)\n",
    "testset = ToySequenceData(n_samples=500, max_seq_len=seq_max_len)\n",
    "\n",
    "#A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "\n",
    "#Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DynamicRNN(x, seqlen, weights, biases):\n",
    "    #Prepare data shape to match 'rnn' function requirements\n",
    "    #Current data input shape: (batch_size, num_steps, num_input)\n",
    "    #Required shape: 'num_steps' tensor list of shape (batch_size, num_input)\n",
    "    \n",
    "    #Unstack to get a list of 'num_steps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, seq_max_len, 1)\n",
    "    print('\\n    x after unstack: ', x, '\\n')\n",
    "    \n",
    "    #Define a lstm cell with a tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_hidden)\n",
    "    \n",
    "    #Get lstm cell output, providing 'sequence_length' will perform dynamic calculation\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32, sequence_length=seqlen)\n",
    "    print('\\n    outputs and states: ', outputs, states, '\\n')\n",
    "    \n",
    "    #When performing dynamic calculation, we must retrieve the last\n",
    "    #dynamically computed output, i.e: if a sequence length is 10, we need to retrieve the 10th output\n",
    "    #However TF doesnt support advanced indexing yet, so we build a custom op that for each sample in batch size\n",
    "    #get its length and get the corresponding relevant ouput\n",
    "    \n",
    "    #'outputs' is a list of output at every timestep, we pack them in a tensor\n",
    "    #and change back dimension to [batch_size, num_step, num_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    print('\\n    outputs after stack: ',outputs, '\\n')\n",
    "    outputs = tf.transpose(outputs, [1,0,2])\n",
    "    print('\\n    outputs after transpose: ',outputs, '\\n')\n",
    "    \n",
    "    #Hack to build the indexing and retrieve the right output\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    print('\\n    batchsize ???? : ', batch_size, '\\n')\n",
    "    \n",
    "    #Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen-1)\n",
    "    print('\\n    index: ',index, '\\n')\n",
    "    #Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, shape=[-1, num_hidden]), index)\n",
    "    print('\\n    Output after gather: ', outputs, '\\n')\n",
    "    \n",
    "    #Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    x after unstack:  [<tf.Tensor 'unstack:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:1' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:2' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:3' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:4' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:5' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:6' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:7' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:8' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:9' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:10' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:11' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:12' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:13' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:14' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:15' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:16' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:17' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:18' shape=(?, 1) dtype=float32>, <tf.Tensor 'unstack:19' shape=(?, 1) dtype=float32>] \n",
      "\n",
      "\n",
      "    outputs and states:  [<tf.Tensor 'rnn/cond/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_1/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_2/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_3/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_4/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_5/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_6/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_7/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_8/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_9/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_10/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_11/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_12/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_13/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_14/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_15/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_16/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_17/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_18/Merge:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'rnn/cond_19/Merge:0' shape=(?, 64) dtype=float32>] LSTMStateTuple(c=<tf.Tensor 'rnn/cond_19/Merge_1:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'rnn/cond_19/Merge_2:0' shape=(?, 64) dtype=float32>) \n",
      "\n",
      "\n",
      "    outputs after stack:  Tensor(\"stack:0\", shape=(20, ?, 64), dtype=float32) \n",
      "\n",
      "\n",
      "    outputs after transpose:  Tensor(\"transpose:0\", shape=(?, 20, 64), dtype=float32) \n",
      "\n",
      "\n",
      "    batchsize ???? :  Tensor(\"strided_slice:0\", shape=(), dtype=int32) \n",
      "\n",
      "\n",
      "    index:  Tensor(\"add:0\", shape=(?,), dtype=int32) \n",
      "\n",
      "\n",
      "    Output after gather:  Tensor(\"Gather:0\", shape=(?, 64), dtype=float32) \n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-e770fc4fd19f>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StrikeWade\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "prediction = DynamicRNN(x, seqlen, weights, biases)\n",
    "\n",
    "#Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss_op)\n",
    "\n",
    "#Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n",
    "\n",
    "#Initialize the global var\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  1 \n",
      "    Minibatch Loss= 0.8794 \n",
      "    Training Accuracy= 0.539\n",
      "Step:  200 \n",
      "    Minibatch Loss= 0.6870 \n",
      "    Training Accuracy= 0.558\n",
      "Step:  400 \n",
      "    Minibatch Loss= 0.6878 \n",
      "    Training Accuracy= 0.529\n",
      "Step:  600 \n",
      "    Minibatch Loss= 0.6878 \n",
      "    Training Accuracy= 0.548\n",
      "Step:  800 \n",
      "    Minibatch Loss= 0.6869 \n",
      "    Training Accuracy= 0.538\n",
      "Step:  1000 \n",
      "    Minibatch Loss= 0.6853 \n",
      "    Training Accuracy= 0.548\n",
      "Step:  1200 \n",
      "    Minibatch Loss= 0.6827 \n",
      "    Training Accuracy= 0.577\n",
      "Step:  1400 \n",
      "    Minibatch Loss= 0.6781 \n",
      "    Training Accuracy= 0.606\n",
      "Step:  1600 \n",
      "    Minibatch Loss= 0.6688 \n",
      "    Training Accuracy= 0.625\n",
      "Step:  1800 \n",
      "    Minibatch Loss= 0.6442 \n",
      "    Training Accuracy= 0.635\n",
      "Step:  2000 \n",
      "    Minibatch Loss= 0.5727 \n",
      "    Training Accuracy= 0.731\n",
      "Step:  2200 \n",
      "    Minibatch Loss= 0.4972 \n",
      "    Training Accuracy= 0.808\n",
      "Step:  2400 \n",
      "    Minibatch Loss= 0.4728 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  2600 \n",
      "    Minibatch Loss= 0.4635 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  2800 \n",
      "    Minibatch Loss= 0.4573 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  3000 \n",
      "    Minibatch Loss= 0.4522 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  3200 \n",
      "    Minibatch Loss= 0.4477 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  3400 \n",
      "    Minibatch Loss= 0.4435 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  3600 \n",
      "    Minibatch Loss= 0.4395 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  3800 \n",
      "    Minibatch Loss= 0.4355 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  4000 \n",
      "    Minibatch Loss= 0.4315 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  4200 \n",
      "    Minibatch Loss= 0.4271 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  4400 \n",
      "    Minibatch Loss= 0.4223 \n",
      "    Training Accuracy= 0.817\n",
      "Step:  4600 \n",
      "    Minibatch Loss= 0.4168 \n",
      "    Training Accuracy= 0.817\n",
      "Step:  4800 \n",
      "    Minibatch Loss= 0.4101 \n",
      "    Training Accuracy= 0.817\n",
      "Step:  5000 \n",
      "    Minibatch Loss= 0.4016 \n",
      "    Training Accuracy= 0.817\n",
      "Step:  5200 \n",
      "    Minibatch Loss= 0.3903 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  5400 \n",
      "    Minibatch Loss= 0.3743 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  5600 \n",
      "    Minibatch Loss= 0.3568 \n",
      "    Training Accuracy= 0.837\n",
      "Step:  5800 \n",
      "    Minibatch Loss= 0.4075 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  6000 \n",
      "    Minibatch Loss= 0.3991 \n",
      "    Training Accuracy= 0.827\n",
      "Step:  6200 \n",
      "    Minibatch Loss= 0.3124 \n",
      "    Training Accuracy= 0.846\n",
      "Step:  6400 \n",
      "    Minibatch Loss= 0.1783 \n",
      "    Training Accuracy= 0.942\n",
      "Step:  6600 \n",
      "    Minibatch Loss= 0.1208 \n",
      "    Training Accuracy= 0.981\n",
      "Step:  6800 \n",
      "    Minibatch Loss= 0.1037 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  7000 \n",
      "    Minibatch Loss= 0.0941 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  7200 \n",
      "    Minibatch Loss= 0.0872 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  7400 \n",
      "    Minibatch Loss= 0.0814 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  7600 \n",
      "    Minibatch Loss= 0.0764 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  7800 \n",
      "    Minibatch Loss= 0.0721 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  8000 \n",
      "    Minibatch Loss= 0.0677 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  8200 \n",
      "    Minibatch Loss= 0.0637 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  8400 \n",
      "    Minibatch Loss= 0.0608 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  8600 \n",
      "    Minibatch Loss= 0.0586 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  8800 \n",
      "    Minibatch Loss= 0.0567 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  9000 \n",
      "    Minibatch Loss= 0.0551 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  9200 \n",
      "    Minibatch Loss= 0.0537 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  9400 \n",
      "    Minibatch Loss= 0.0525 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  9600 \n",
      "    Minibatch Loss= 0.0513 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  9800 \n",
      "    Minibatch Loss= 0.0503 \n",
      "    Training Accuracy= 0.971\n",
      "Step:  10000 \n",
      "    Minibatch Loss= 0.0494 \n",
      "    Training Accuracy= 0.971\n",
      "DynamicRNN Training Completed !!!!\n",
      "Testing Accuracy:  0.964\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "training_loss = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        \n",
    "        #Run optimization (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, seqlen: batch_seqlen})\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            #Calculate batch accuracy & loss\n",
    "            acc, loss = sess.run([accuracy, loss_op], feed_dict={x: batch_x, y: batch_y, seqlen: batch_seqlen})\n",
    "            \n",
    "            training_loss.append(loss)\n",
    "            \n",
    "            print('Step: ', str(step), '\\n    Minibatch Loss= {:.4f}'.format(loss), '\\n    Training Accuracy= {:.3f}'.format(acc))\n",
    "            \n",
    "    print('DynamicRNN Training Completed !!!!')\n",
    "    \n",
    "    #Calculate accuracy\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "    print('Testing Accuracy: ', sess.run(accuracy, feed_dict={x: test_data, y: test_label, seqlen: test_seqlen}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23546cd2240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
